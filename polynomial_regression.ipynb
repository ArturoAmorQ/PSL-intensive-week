{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53e476c",
   "metadata": {},
   "source": [
    "# Regressions in physics with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95619941",
   "metadata": {},
   "source": [
    "In this notebook we will look at some posibilities for fitting data and reproducing the data-generating procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3749ae",
   "metadata": {},
   "source": [
    "## Create synthetic polynomial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_sample = 100\n",
    "data_max, data_min = 1.4, -1.4\n",
    "len_data = (data_max - data_min)\n",
    "# sort the data to make plotting easier later\n",
    "data = np.sort(rng.rand(n_sample) * len_data - len_data / 2)\n",
    "noise = rng.randn(n_sample) * .5\n",
    "target = data ** 3 - 0.5 * data ** 2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3792acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_data = pd.DataFrame({\"input_feature\": data, \"target\": target})\n",
    "true_coefs = pd.DataFrame({'intercept': [0.], 'weight_X': [0.],\n",
    "                           'weight_X^2':[-0.5], 'weight_X^3': [1.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                    color=\"black\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26c1b7",
   "metadata": {},
   "source": [
    "In scikit-learn, by convention data (also called X in the scikit-learn documentation) should be a 2D matrix of shape (n_samples, n_features). If data is a 1D vector, you need to reshape it into a matrix with a single column if the vector represents a feature or a single row if the vector represents a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X should be 2D for sklearn: (n_samples, n_features)\n",
    "data = data.reshape((-1, 1))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59436c",
   "metadata": {},
   "source": [
    "## Try different regressions\n",
    "\n",
    "A common first step would be to try the simplest option: fitting a straight line as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data, target)\n",
    "target_predicted = linear_regression.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a855ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ade40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"weight: {linear_regression.coef_[0]:.2f}, \"\n",
    "      f\"intercept: {linear_regression.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7a454",
   "metadata": {},
   "source": [
    "It is important to note that the learnt model will not be able to handle the non-linear relationship between data and target since linear models assume the relationship between data and target to be linear.\n",
    "\n",
    "Indeed, there are 3 possibilities to solve this issue:\n",
    "\n",
    "    - choose a model that can natively deal with non-linearity,\n",
    "\n",
    "    - engineer a richer set of features by including expert knowledge which can be directly used by a simple linear model, or\n",
    "\n",
    "    - use a “kernel” to have a locally-based decision function instead of a global linear decision function.\n",
    "\n",
    "Let’s illustrate quickly the first point by using a decision tree regressor which can natively handle non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(data, target)\n",
    "target_predicted = tree.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a65d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32af28",
   "metadata": {},
   "source": [
    "The problem with this approach is interpretability of the model in terms of first principles. The same applies to using a kernel.\n",
    "\n",
    "Instead we could also modify our data: we could create new features, derived from the original features, using some expert knowledge. In this example, we know that we have a cubic and squared relationship between data and target (because we generated the data).\n",
    "\n",
    "Indeed, we could create two new features (data ** 2 and data ** 3) using this information as follows. This kind of transformation is called a polynomial feature expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expanded = np.concatenate([data, data ** 2, data ** 3], axis=1)\n",
    "data_expanded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499397f6",
   "metadata": {},
   "source": [
    "A polynomial regression is nothing but a linear regression with polynomial features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342da1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.fit(data_expanded, target)\n",
    "target_predicted = linear_regression.predict(data_expanded)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bcd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3330b2",
   "metadata": {},
   "source": [
    "We can see that even with a linear model, we can overcome the linearity limitation of the model by adding the non-linear components in the design of additional features. Here, we created new features by knowing the way the target was generated.\n",
    "\n",
    "Instead of manually creating such polynomial features one could directly use [`sklearn.preprocessing.PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "To demonstrate the use of the `PolynomialFeatures` class, we use a scikit-learn pipeline which first transforms the features and then fit the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fe340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=False),\n",
    "    LinearRegression(),\n",
    ")\n",
    "polynomial_regression.fit(data, target)\n",
    "target_predicted = polynomial_regression.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608a1ce",
   "metadata": {},
   "source": [
    "Generate a new feature matrix consisting of all polynomial combinations\n",
    "of the features with degree less than or equal to the specified degree.\n",
    "In this case, the `degree=3` polynomial features are [1, x, x^2, x^3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"intercept: {polynomial_regression[-1].intercept_:.2f}, \"\n",
    "      f\"weight_X: {polynomial_regression[-1].coef_[0]:.2f}, \"\n",
    "      f\"weight_X^2: {polynomial_regression[-1].coef_[1]:.2f}, \"\n",
    "      f\"weight_X^3: {polynomial_regression[-1].coef_[2]:.2f}, \"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bdc251",
   "metadata": {},
   "source": [
    "We recall that `target` was created using `target = data ** 3 - 0.5 * data ** 2 + noise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb5a4c",
   "metadata": {},
   "source": [
    "As this procedure is equivalent to creating the features by hand (up to numerical error), it should not be surprising that the predictions of the `PolynomialFeatures` pipeline match the predictions of the linear model fit on manually engineered features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209a7af",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Here we use cross-validation to estimate the stability of the weights found by the regression. First we get a visual intution of the `KFold` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "X_plot = np.linspace(-1.5,1.5,20)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(data):\n",
    "    if i == 0:\n",
    "        plt.scatter(full_data[\"input_feature\"], full_data[\"target\"],\n",
    "        color=\"black\", s=60,\n",
    "        alpha=1, label=\"Original data\")\n",
    "        plt.title(\"Full data before cross-validation split\", fontsize=14)\n",
    "    i=i+1\n",
    "    \n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    polynomial_regression.fit(X_train, y_train)\n",
    "    y_predicted = polynomial_regression.predict(X_plot.reshape(-1, 1))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(f\"Fitted curve in Fold #{i} of KFold(shuffle=True)\", fontsize=14)\n",
    "    ax.scatter(X_test, y_test,\n",
    "            color=\"tab:blue\", facecolors=\"none\",\n",
    "            alpha=0.5, label=\"Resampled data\", s=180, linewidth=5)\n",
    "    ax.scatter(full_data[\"input_feature\"], full_data[\"target\"],\n",
    "            color=\"black\", s=60,\n",
    "            alpha=1, label=\"Original data\")\n",
    "\n",
    "\n",
    "    ax.plot(X_plot, y_predicted, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9b41c",
   "metadata": {},
   "source": [
    "The `KFold` strategy will select `K` non-overlapping subsets of the data for testing. In this case we set `K=5` by passing the parameter `n_splits=5`, meaning that each fold will contain 20% of the data.\n",
    "\n",
    "The `ShuffleSplit` strategy will draw subsets of possibly overlapping data for testing in each iteration. In this case the number of splits is independent of the `test_size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=40, test_size=0.1, random_state=0)\n",
    "\n",
    "cv_results = cross_validate(polynomial_regression, data, target,\n",
    "                            cv=cv, return_estimator=True,\n",
    "                            scoring='neg_mean_squared_error')\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08737d49",
   "metadata": {},
   "source": [
    "We get a different score in each fold. We have a **score distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80525855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_results[\"test_error\"].plot.hist(bins=10, edgecolor=\"black\", density=True)\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Test error distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validated testing error is: \"\n",
    "      f\"{cv_results['test_error'].mean():.2f} +/-\"\n",
    "      f\"{cv_results['test_error'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09502e2a",
   "metadata": {},
   "source": [
    "As we have different fits for different folds, we also have **weights distributions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94298d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [pipeline[-1].intercept_ for pipeline in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=[\"intercept\"])\n",
    "\n",
    "coefs[\"weight_X\"] = [pipeline[-1].coef_[0] for pipeline in cv_results[\"estimator\"]]\n",
    "coefs[\"weight_X^2\"] = [pipeline[-1].coef_[1] for pipeline in cv_results[\"estimator\"]]\n",
    "coefs[\"weight_X^3\"] = [pipeline[-1].coef_[2] for pipeline in cv_results[\"estimator\"]]\n",
    "\n",
    "print(f\"intercept: {coefs['intercept'].mean():.2f} +/-\"\n",
    "      f\"{coefs['intercept'].std():.2f}, \"\n",
    "      f\"weight_X: {coefs['weight_X'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X'].std():.2f}, \"\n",
    "      f\"weight_X^2: {coefs['weight_X^2'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X^2'].std():.2f}, \"\n",
    "      f\"weight_X^3: {coefs['weight_X^3'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X^3'].std():.2f}, \"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d260551",
   "metadata": {},
   "source": [
    "We could also use a histogram to visualize each coefficient, but a more intuitive way to display the set of equally valid fits is by using boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = sns.boxplot(data=true_coefs, orient=\"h\", color='red', medianprops=dict(color=\"red\"))\n",
    "ax1 = sns.boxplot(ax=ax0, data=coefs, orient=\"h\", color='blue')\n",
    "\n",
    "\n",
    "_ = plt.title(\"Linear regression coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = np.linspace(-1.6, 1.6, 25)\n",
    "\n",
    "for i in range(0,40):\n",
    "    model = cv_results[\"estimator\"][i]\n",
    "    predictions = model.predict(X_plot.reshape(-1, 1))\n",
    "    plt.plot(X_plot, predictions, linestyle=\"--\", alpha=0.1,\n",
    "             color=\"tab:blue\")\n",
    "\n",
    "plt.scatter(full_data[\"input_feature\"], full_data[\"target\"],\n",
    "            color=\"black\", s=60,\n",
    "            alpha=1)\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86404da",
   "metadata": {},
   "source": [
    "A confidence interval is the zone covered by the different sets of coefficients. Unfortunately, not all the scikit-learn regressions have implemented the option to provide the confidence interval, but tools such as `seaborn.regplot` do the job using bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce74c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Fitted curve with confidence interval\", fontsize=16)\n",
    "ax.set(xlim=(-1.5,1.5))\n",
    "sns.regplot(data=full_data, x=\"input_feature\", y=\"target\", order=3,\n",
    "           color=\"tab:blue\", ax=ax)\n",
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", s=100)\n",
    "#_ = plt.title(\"Bootstrap confidence interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3012f",
   "metadata": {},
   "source": [
    "From the boxplot we see that our model is not capturing the ground truth for the `intercept` nor the `weight_X`.\n",
    "\n",
    "## Linear models with regularization\n",
    "\n",
    "Examples of such models are [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) and [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-1, -0.2, num=50)\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=False),\n",
    "    RidgeCV(alphas=alphas, store_cv_values=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e95f35",
   "metadata": {},
   "source": [
    "`RidgeCV` will automatically tune the regularization parameter using cross-validation, but one must keep an external, held-out test set for the final evaluation of the refitted model. We should use an additional cross-validation for this evaluation. This pattern is called nested cross-validation. In practice, we only need to embed the pipeline in the function `cross_validate` to perform such evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(polynomial_regression, data, target,\n",
    "                            cv=cv, scoring=\"neg_mean_squared_error\",\n",
    "                            return_estimator=True, n_jobs=2)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_alphas = [est[-1].cv_values_.mean(axis=0)\n",
    "              for est in cv_results[\"estimator\"]]\n",
    "cv_alphas = pd.DataFrame(mse_alphas, columns=alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_alphas.mean(axis=0).plot(marker=\"+\")\n",
    "plt.ylabel(\"Mean squared error\\n (lower is better)\")\n",
    "plt.xlabel(\"alpha\")\n",
    "_ = plt.title(\"Error obtained by cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8fd6e",
   "metadata": {},
   "source": [
    "We found a sweetspot for the hyperparameter `alpha`. We can further question how stable is this sweetspot through the different trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alphas = [est[-1].alpha_ for est in cv_results[\"estimator\"]]\n",
    "print(f\"The range of alphas leading to the best generalization performance is:\\n\"\n",
    "      f\"{np.mean(best_alphas):.2f} +/- {np.std(best_alphas):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validated testing error is: \"\n",
    "      f\"{cv_results['test_error'].mean():.3f} +/-\"\n",
    "      f\"{cv_results['test_error'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [pipeline[-1].intercept_ for pipeline in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=[\"intercept\"])\n",
    "\n",
    "coefs[\"weight_X\"] = [pipeline[-1].coef_[0] for pipeline in cv_results[\"estimator\"]]\n",
    "coefs[\"weight_X^2\"] = [pipeline[-1].coef_[1] for pipeline in cv_results[\"estimator\"]]\n",
    "coefs[\"weight_X^3\"] = [pipeline[-1].coef_[2] for pipeline in cv_results[\"estimator\"]]\n",
    "\n",
    "print(f\"intercept: {coefs['intercept'].mean():.2f} +/-\"\n",
    "      f\"{coefs['intercept'].std():.2f}, \"\n",
    "      f\"weight_X: {coefs['weight_X'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X'].std():.2f}, \"\n",
    "      f\"weight_X^2: {coefs['weight_X^2'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X^2'].std():.2f}, \"\n",
    "      f\"weight_X^3: {coefs['weight_X^3'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X^3'].std():.2f}, \"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6640e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = sns.boxplot(data=true_coefs, orient=\"h\", color='red', medianprops=dict(color=\"red\"))\n",
    "ax1 = sns.boxplot(ax=ax0, data=coefs, orient=\"h\", color='blue')\n",
    "\n",
    "_ = plt.title(\"Ridge regression coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887236ba",
   "metadata": {},
   "source": [
    "Notice that `intercept` and `weight_X` are a bit shifted towards zero, but the `RidgeCV` model is still not capturing the ground truth for those coefficients. `LassoCV` uses a different kind of regularization that is able to set the weights of a fit exactly to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=False),\n",
    "    LassoCV(alphas=alphas, cv=cv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6eb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(polynomial_regression, data, target,\n",
    "                            cv=cv, scoring=\"neg_mean_squared_error\",\n",
    "                            return_estimator=True, n_jobs=2)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]\n",
    "\n",
    "mse_alphas = [est[-1].alpha_ for est in cv_results[\"estimator\"]]\n",
    "cv_results['alphas'] = pd.DataFrame(mse_alphas)\n",
    "\n",
    "cv_results.plot.scatter(x='alphas', y='test_error')\n",
    "\n",
    "_ = plt.title(\"LassoCV best alphas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a733252",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression[-1].set_params(alphas=cv_results['alphas'].unique())\n",
    "polynomial_regression.fit(data, target)\n",
    "target_predicted = polynomial_regression.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c324cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [pipeline[-1].intercept_ for pipeline in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=[\"intercept\"])\n",
    "\n",
    "coefs[\"weight_X\"] = [pipeline[-1].coef_[0] for pipeline in cv_results[\"estimator\"]]\n",
    "coefs[\"weight_X^2\"] = [pipeline[-1].coef_[1] for pipeline in cv_results[\"estimator\"]]\n",
    "coefs[\"weight_X^3\"] = [pipeline[-1].coef_[2] for pipeline in cv_results[\"estimator\"]]\n",
    "\n",
    "print(f\"intercept: {coefs['intercept'].mean():.2f} +/-\"\n",
    "      f\"{coefs['intercept'].std():.2f}, \"\n",
    "      f\"weight_X: {coefs['weight_X'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X'].std():.2f}, \"\n",
    "      f\"weight_X^2: {coefs['weight_X^2'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X^2'].std():.2f}, \"\n",
    "      f\"weight_X^3: {coefs['weight_X^3'].mean():.2f} +/-\"\n",
    "      f\"{coefs['weight_X^3'].std():.2f}, \"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f876bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax0 = sns.boxplot(data=true_coefs, orient=\"h\", color='red', medianprops=dict(color=\"red\"))\n",
    "ax1 = sns.boxplot(ax=ax0, data=coefs, orient=\"h\", color='blue', medianprops=dict(alpha=0.5))\n",
    "\n",
    "_ = plt.title(\"Lasso regression coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b47473",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Try other linear models with integrated parameter tuning such as [`ElasticNetCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html). Alternatively, try Bayesian linear models such as [`BayesianRidge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html) or [`ARDRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html)\n",
    "- Try modifying the syntethic dataset: add noise, change its distribution, use linear combinations of trigonometric functions, exponentials, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffeb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('scikit-learn-course': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd011c93f6972e2fb3ae1edd39dd4c57f14f9f122abf4abdcbf02ecd68eec34d570"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
